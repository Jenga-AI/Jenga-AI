{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Jenga-AI Testing Framework in Google Colab\n",
        "\n",
        "This notebook provides a complete testing environment for the Jenga-AI multi-task NLP framework.\n",
        "\n",
        "**Prerequisites:**\n",
        "1. Upload `jenga-ai-core.zip` to your Google Drive\n",
        "2. Enable GPU runtime: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "3. Run cells in order\n",
        "\n",
        "**Expected Runtime:** 10-15 minutes total"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ Step 1: Setup and Extract Project"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted successfully\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Jenga-AI project\n",
        "# ‚ö†Ô∏è UPDATE THIS PATH to where you uploaded jenga-ai-core.zip in your Drive\n",
        "zip_path = '/content/drive/MyDrive/jenga-ai-core.zip'  # Change this path!\n",
        "\n",
        "# Check if file exists\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"‚úÖ Found ZIP file: {zip_path}\")\n",
        "    \n",
        "    # Extract project\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "    \n",
        "    # Change to project directory\n",
        "    %cd /content/jenga-ai-colab\n",
        "    \n",
        "    print(\"üìÅ Project extracted successfully!\")\n",
        "    !ls -la\n",
        "else:\n",
        "    print(f\"‚ùå ZIP file not found at: {zip_path}\")\n",
        "    print(\"Please upload jenga-ai-core.zip to your Google Drive and update the path above\")"
      ],
      "metadata": {
        "id": "extract_project"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Step 2: Install Dependencies and Setup"
      ],
      "metadata": {
        "id": "deps_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run automated Colab setup\n",
        "!python setup_colab.py"
      ],
      "metadata": {
        "id": "run_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify GPU and environment\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "print(\"üîç Environment Check:\")\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
        "    print(f\"   CUDA: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU not detected - ensure GPU runtime is enabled\")\n",
        "\n",
        "print(f\"\\nüìÅ Current directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "verify_env"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Step 3: Run Testing Framework"
      ],
      "metadata": {
        "id": "test_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run environment check\n",
        "!python tests/environment_check.py"
      ],
      "metadata": {
        "id": "env_check"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run unit tests (fast - 2-3 minutes)\n",
        "print(\"üß™ Running Unit Tests...\")\n",
        "!python tests/run_test_suite.py --unit-only --verbose"
      ],
      "metadata": {
        "id": "unit_tests"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test individual components\n",
        "print(\"üîç Testing Import Validation:\")\n",
        "!python tests/unit/test_imports.py"
      ],
      "metadata": {
        "id": "import_tests"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data loading capabilities\n",
        "print(\"üìä Testing Data Loading:\")\n",
        "!python tests/unit/test_data_loading.py"
      ],
      "metadata": {
        "id": "data_tests"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model components\n",
        "print(\"ü§ñ Testing Model Components:\")\n",
        "!python tests/unit/test_attention_fusion.py"
      ],
      "metadata": {
        "id": "model_tests"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Step 4: Integration Tests (Optional - Longer Runtime)"
      ],
      "metadata": {
        "id": "integration_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run integration tests (5-10 minutes)\n",
        "# ‚ö†Ô∏è This will take longer but tests end-to-end training\n",
        "print(\"üîÑ Running Integration Tests...\")\n",
        "!python tests/run_test_suite.py --integration --verbose"
      ],
      "metadata": {
        "id": "integration_tests"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test single-task training workflow\n",
        "print(\"üìà Testing Training Pipeline:\")\n",
        "!python tests/integration/test_single_task_training.py"
      ],
      "metadata": {
        "id": "training_tests"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Step 5: Complete Test Suite (Full Validation)"
      ],
      "metadata": {
        "id": "complete_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run complete test suite with detailed reporting\n",
        "print(\"üéØ Running Complete Test Suite...\")\n",
        "!python tests/run_test_suite.py --all --json test_results.json"
      ],
      "metadata": {
        "id": "complete_tests"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze test results\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load test results\n",
        "if os.path.exists('test_results.json'):\n",
        "    with open('test_results.json', 'r') as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(\"üìà Test Results Summary:\")\n",
        "    summary = results['summary']\n",
        "    print(f\"Total Tests: {summary['total_tests']}\")\n",
        "    print(f\"Passed: {summary['total_passed']} ({100*summary['total_passed']/summary['total_tests']:.1f}%)\")\n",
        "    print(f\"Failed: {summary['total_failed']}\")\n",
        "    print(f\"Errors: {summary['total_errors']}\")\n",
        "    print(f\"Duration: {results['duration']:.1f} seconds\")\n",
        "    \n",
        "    # Create DataFrame for detailed analysis\n",
        "    module_data = []\n",
        "    for module, data in results['modules'].items():\n",
        "        module_data.append({\n",
        "            'Module': module.split('.')[-1],  # Just the test file name\n",
        "            'Passed': data['passed'],\n",
        "            'Failed': data['failed'], \n",
        "            'Errors': data['errors'],\n",
        "            'Duration': f\"{data['duration']:.2f}s\"\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(module_data)\n",
        "    print(\"\\nüìä Per-Module Results:\")\n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Test results file not found. Run the complete test suite first.\")"
      ],
      "metadata": {
        "id": "analyze_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Step 6: Custom Testing and Exploration"
      ],
      "metadata": {
        "id": "custom_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick manual test of core functionality\n",
        "print(\"üî¨ Manual Framework Test:\")\n",
        "\n",
        "try:\n",
        "    # Test basic imports\n",
        "    from multitask_bert.core.config import ExperimentConfig, TaskConfig, HeadConfig\n",
        "    from multitask_bert.data.data_processing import DataProcessor\n",
        "    from transformers import AutoTokenizer\n",
        "    \n",
        "    print(\"‚úÖ Core imports successful\")\n",
        "    \n",
        "    # Test tokenizer loading\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "    print(\"‚úÖ Tokenizer loaded successfully\")\n",
        "    \n",
        "    # Test basic tokenization\n",
        "    text = \"This is a test sentence for Jenga-AI.\"\n",
        "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=64)\n",
        "    print(f\"‚úÖ Tokenization successful: {len(tokens['input_ids'])} tokens\")\n",
        "    \n",
        "    print(\"\\nüéâ Core functionality working!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in manual test: {e}\")"
      ],
      "metadata": {
        "id": "manual_test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monitor GPU memory usage\n",
        "if torch.cuda.is_available():\n",
        "    print(\"üíæ GPU Memory Status:\")\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
        "    print(f\"Cached: {torch.cuda.memory_reserved()/1e9:.2f}GB\")\n",
        "    print(f\"Max Allocated: {torch.cuda.max_memory_allocated()/1e9:.2f}GB\")\n",
        "    \n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"üßπ GPU cache cleared\")\n",
        "else:\n",
        "    print(\"üíª Running in CPU mode\")"
      ],
      "metadata": {
        "id": "monitor_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Step 7: Results Summary and Next Steps"
      ],
      "metadata": {
        "id": "summary_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üéØ Jenga-AI Testing Complete!\")\n",
        "print(\"=\"*50)\n",
        "print()\n",
        "print(\"üìä What was tested:\")\n",
        "print(\"‚Ä¢ Module imports and dependencies\")\n",
        "print(\"‚Ä¢ Data loading (CSV, JSON, JSONL formats)\")\n",
        "print(\"‚Ä¢ Data preprocessing and tokenization\")\n",
        "print(\"‚Ä¢ Model components (attention fusion, multi-task architecture)\")\n",
        "print(\"‚Ä¢ Configuration parsing and validation\")\n",
        "print(\"‚Ä¢ Training workflows (single-task and integration)\")\n",
        "print()\n",
        "print(\"üîç Files generated:\")\n",
        "!ls -la | grep -E \"\\.(json|log)$\" || echo \"No output files found\"\n",
        "print()\n",
        "print(\"üöÄ Next Steps:\")\n",
        "print(\"1. Review test results above for any failures\")\n",
        "print(\"2. Document any bugs or issues discovered\")\n",
        "print(\"3. Experiment with the framework using your own data\")\n",
        "print(\"4. Contribute findings back to the Jenga-AI project\")\n",
        "print()\n",
        "print(\"üìö Documentation:\")\n",
        "print(\"‚Ä¢ TESTING_IMPLEMENTATION_REPORT.md - Complete framework overview\")\n",
        "print(\"‚Ä¢ TESTING_QUICK_START.md - Local testing guide\")\n",
        "print(\"‚Ä¢ COLAB_QUICK_START.md - This Colab setup guide\")\n",
        "print()\n",
        "print(\"üéâ Happy testing with Jenga-AI!\")"
      ],
      "metadata": {
        "id": "final_summary"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}