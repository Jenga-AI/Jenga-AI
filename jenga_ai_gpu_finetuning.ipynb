{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Jenga-AI GPU Finetuning on Google Colab\n",
    "\n",
    "This notebook provides a complete pipeline for finetuning LLMs using the Jenga-AI framework with GPU acceleration on Google Colab.\n",
    "\n",
    "**Runtime Requirements:**\n",
    "- Go to `Runtime` ‚Üí `Change runtime type`\n",
    "- Select `T4 GPU` as Hardware accelerator\n",
    "- Click `Save`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if we have GPU\n",
    "gpu_info = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "if gpu_info.returncode == 0:\n",
    "    print(\"‚úÖ GPU Available:\")\n",
    "    print(gpu_info.stdout)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Please enable GPU runtime in Colab.\")\n",
    "    print(\"Go to Runtime ‚Üí Change runtime type ‚Üí Select T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers accelerate peft datasets mlflow bitsandbytes scipy\n",
    "!pip install -q huggingface_hub tokenizers safetensors tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations\n",
    "import torch\n",
    "import transformers\n",
    "import mlflow\n",
    "import peft\n",
    "\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ Transformers version: {transformers.__version__}\")\n",
    "print(f\"‚úÖ PEFT version: {peft.__version__}\")\n",
    "print(f\"‚úÖ MLflow version: {mlflow.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Synthetic Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "def generate_synthetic_data(num_samples: int = 500) -> List[Dict]:\n",
    "    \"\"\"Generate synthetic conversational data for training\"\"\"\n",
    "    \n",
    "    # Templates for generating diverse conversations\n",
    "    contexts = [\n",
    "        \"customer support\",\n",
    "        \"technical assistance\",\n",
    "        \"general inquiry\",\n",
    "        \"complaint resolution\",\n",
    "        \"product information\"\n",
    "    ]\n",
    "    \n",
    "    greetings = [\n",
    "        \"Hello, how can I help you today?\",\n",
    "        \"Welcome! What can I assist you with?\",\n",
    "        \"Good day! How may I help you?\",\n",
    "        \"Thank you for calling. What brings you here today?\"\n",
    "    ]\n",
    "    \n",
    "    issues = [\n",
    "        \"I'm having trouble with my account\",\n",
    "        \"I need information about your services\",\n",
    "        \"There's a problem with my recent order\",\n",
    "        \"I'd like to know more about pricing\",\n",
    "        \"Can you help me with a technical issue?\"\n",
    "    ]\n",
    "    \n",
    "    responses = [\n",
    "        \"I understand your concern. Let me help you with that.\",\n",
    "        \"I'll be happy to assist you with this issue.\",\n",
    "        \"Thank you for bringing this to our attention.\",\n",
    "        \"Let me look into that for you right away.\"\n",
    "    ]\n",
    "    \n",
    "    resolutions = [\n",
    "        \"I've resolved the issue for you. Is there anything else?\",\n",
    "        \"The problem has been fixed. Please let me know if you need more help.\",\n",
    "        \"Everything should be working now. Thank you for your patience.\",\n",
    "        \"I've updated your information. The changes will take effect shortly.\"\n",
    "    ]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Create a conversation\n",
    "        conversation = [\n",
    "            random.choice(greetings),\n",
    "            random.choice(issues),\n",
    "            random.choice(responses),\n",
    "            \"Can you provide more details about your issue?\",\n",
    "            \"Yes, \" + random.choice([\n",
    "                \"it started happening yesterday\",\n",
    "                \"I've tried restarting but it didn't help\",\n",
    "                \"this has been ongoing for a week\",\n",
    "                \"I followed the instructions but still have problems\"\n",
    "            ]),\n",
    "            random.choice(resolutions),\n",
    "            \"Thank you for your help!\",\n",
    "            \"You're welcome! Have a great day!\"\n",
    "        ]\n",
    "        \n",
    "        # Join into text\n",
    "        text = \"\\n\".join([f\"{['Agent', 'Customer'][j%2]}: {msg}\" \n",
    "                         for j, msg in enumerate(conversation)])\n",
    "        \n",
    "        data.append({\n",
    "            \"text\": text,\n",
    "            \"sample_id\": f\"synthetic_{i:04d}\",\n",
    "            \"context\": random.choice(contexts),\n",
    "            \"quality_level\": random.choice([\"good\", \"excellent\", \"fair\"]),\n",
    "            \"length\": len(text.split())\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"üîÑ Generating synthetic training data...\")\n",
    "synthetic_data = generate_synthetic_data(500)\n",
    "print(f\"‚úÖ Generated {len(synthetic_data)} synthetic samples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìù Sample data:\")\n",
    "print(synthetic_data[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup MLflow\n",
    "mlflow.set_tracking_uri(\"file:///content/mlruns\")\n",
    "experiment_name = \"gpu-llm-finetuning\"\n",
    "\n",
    "# Create or get experiment\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        experiment_name,\n",
    "        artifact_location=\"/content/mlruns\"\n",
    "    )\n",
    "except:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"‚úÖ MLflow experiment: {experiment_name}\")\n",
    "print(f\"‚úÖ Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': encoding['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "model_name = \"microsoft/DialoGPT-small\"  # Good for conversations\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.9 * len(synthetic_data))\n",
    "train_data = synthetic_data[:train_size]\n",
    "val_data = synthetic_data[train_size:]\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_data)}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ConversationDataset(train_data, tokenizer, max_length=256)\n",
    "val_dataset = ConversationDataset(val_data, tokenizer, max_length=256)\n",
    "\n",
    "print(f\"‚úÖ Datasets created with max_length=256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Model with PEFT/LoRA for Efficient Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "print(\"üîÑ Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for T4 GPU\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]  # DialoGPT attention layers\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on {device}\")\n",
    "print(f\"‚úÖ Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")\n",
    "print(f\"‚úÖ Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Training arguments optimized for T4 GPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # T4 can handle batch size 4-8\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # GPU optimization\n",
    "    fp16=True,  # Mixed precision training\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # MLflow\n",
    "    report_to=[\"mlflow\"],\n",
    "    run_name=f\"gpu-finetune-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \n",
    "    # Other\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured for T4 GPU\")\n",
    "print(f\"üìä Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"üìä Total training steps: ~{len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"This will take approximately 5-10 minutes on T4 GPU\\n\")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run() as run:\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"model_name\": model_name,\n",
    "        \"num_train_samples\": len(train_dataset),\n",
    "        \"num_val_samples\": len(val_dataset),\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"num_epochs\": training_args.num_train_epochs,\n",
    "        \"lora_r\": peft_config.r,\n",
    "        \"lora_alpha\": peft_config.lora_alpha,\n",
    "        \"max_length\": 256,\n",
    "        \"device\": str(device),\n",
    "    })\n",
    "    \n",
    "    # Train\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Log final metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"final_train_loss\": train_result.metrics[\"train_loss\"],\n",
    "        \"total_steps\": train_result.metrics[\"train_steps\"],\n",
    "        \"training_time\": train_result.metrics[\"train_runtime\"],\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training completed!\")\n",
    "    print(f\"üìä Final loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    print(f\"‚è±Ô∏è Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"üîó MLflow run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"üîÑ Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_id=run.info.run_id):\n",
    "    mlflow.log_metrics({\n",
    "        f\"eval_{key}\": value \n",
    "        for key, value in eval_results.items()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Generated Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model, tokenizer, max_length=100):\n",
    "    \"\"\"Generate a response from the fine-tuned model\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Agent: Hello, how can I help you today?\\nCustomer: I'm having trouble with my account\\nAgent:\",\n",
    "    \"Agent: Welcome! What can I assist you with?\\nCustomer: I need information about pricing\\nAgent:\",\n",
    "    \"Agent: Thank you for calling. How may I help?\\nCustomer: There's a technical issue\\nAgent:\",\n",
    "]\n",
    "\n",
    "print(\"ü§ñ Testing fine-tuned model responses:\\n\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    response = generate_response(prompt, model, tokenizer)\n",
    "    print(f\"Response: {response}\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "output_dir = \"./finetuned_model\"\n",
    "\n",
    "print(\"üíæ Saving fine-tuned model...\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {output_dir}\")\n",
    "\n",
    "# Save training configuration\n",
    "import json\n",
    "config_data = {\n",
    "    \"model_name\": model_name,\n",
    "    \"training_args\": training_args.to_dict(),\n",
    "    \"peft_config\": {\n",
    "        \"r\": peft_config.r,\n",
    "        \"lora_alpha\": peft_config.lora_alpha,\n",
    "        \"lora_dropout\": peft_config.lora_dropout,\n",
    "        \"target_modules\": peft_config.target_modules,\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"final_train_loss\": train_result.metrics[\"train_loss\"],\n",
    "        \"eval_loss\": eval_results[\"eval_loss\"],\n",
    "        \"training_time\": train_result.metrics[\"train_runtime\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Configuration saved to: {output_dir}/training_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Create Model Card and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = f\"\"\"# Fine-tuned Conversational Model\n",
    "\n",
    "## Model Details\n",
    "- **Base Model**: {model_name}\n",
    "- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)\n",
    "- **Training Device**: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
    "- **Training Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "## Training Configuration\n",
    "- **Training Samples**: {len(train_dataset)}\n",
    "- **Validation Samples**: {len(val_dataset)}\n",
    "- **Batch Size**: {training_args.per_device_train_batch_size}\n",
    "- **Learning Rate**: {training_args.learning_rate}\n",
    "- **Epochs**: {training_args.num_train_epochs}\n",
    "- **LoRA Rank**: {peft_config.r}\n",
    "- **LoRA Alpha**: {peft_config.lora_alpha}\n",
    "\n",
    "## Performance Metrics\n",
    "- **Final Training Loss**: {train_result.metrics['train_loss']:.4f}\n",
    "- **Evaluation Loss**: {eval_results['eval_loss']:.4f}\n",
    "- **Training Time**: {train_result.metrics['train_runtime']:.2f} seconds\n",
    "- **Trainable Parameters**: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained('{model_name}')\n",
    "model = PeftModel.from_pretrained(model, './finetuned_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./finetuned_model')\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(\"Your prompt here\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "## Training Framework\n",
    "- Jenga-AI LLM Fine-tuning Pipeline\n",
    "- Google Colab T4 GPU Runtime\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(f\"{output_dir}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"üìÑ Model Card:\")\n",
    "print(model_card)\n",
    "print(f\"\\n‚úÖ Model card saved to: {output_dir}/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Download Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model for download\n",
    "!zip -r finetuned_model.zip finetuned_model/\n",
    "\n",
    "print(\"‚úÖ Model compressed to: finetuned_model.zip\")\n",
    "print(\"üì• You can now download the model from the Files tab\")\n",
    "\n",
    "# Also zip MLflow runs\n",
    "!zip -r mlflow_runs.zip mlruns/\n",
    "print(\"‚úÖ MLflow runs compressed to: mlflow_runs.zip\")\n",
    "\n",
    "# Display download links in Colab\n",
    "from google.colab import files\n",
    "print(\"\\nüì• Click below to download:\")\n",
    "files.download('finetuned_model.zip')\n",
    "files.download('mlflow_runs.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Training Summary\n",
    "\n",
    "Congratulations! You've successfully fine-tuned an LLM using the Jenga-AI framework on Google Colab's T4 GPU.\n",
    "\n",
    "### Key Achievements:\n",
    "- ‚úÖ Generated 500 synthetic training samples\n",
    "- ‚úÖ Fine-tuned model with LoRA for efficiency\n",
    "- ‚úÖ Achieved training on T4 GPU in ~5-10 minutes\n",
    "- ‚úÖ Integrated MLflow for experiment tracking\n",
    "- ‚úÖ Saved model for deployment\n",
    "\n",
    "### Next Steps:\n",
    "1. **Deploy the model**: Use the saved model for inference\n",
    "2. **Experiment with hyperparameters**: Try different learning rates, batch sizes\n",
    "3. **Use real data**: Replace synthetic data with your actual dataset\n",
    "4. **Scale up**: Try larger models like `gpt2-medium` or `llama-2-7b`\n",
    "\n",
    "### Resources:\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}